{
 "cells": [
  {
   "cell_type": "code",
   "id": "836218c5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T15:46:20.875017Z",
     "start_time": "2025-07-23T15:46:20.862594Z"
    }
   },
   "source": [
    "from mss import mss # used for screen capture and faster than cv in this work\n",
    "import pydirectinput # sends command to chrome/ alternative of selenium\n",
    "import cv2 # frame processing\n",
    "import numpy as np # transformation of framework\n",
    "import pytesseract # extracts text from image\n",
    "from matplotlib import pyplot as plt # vissualize captured frames\n",
    "import time # for pauses\n",
    "from gymnasium import Env # environment components\n",
    "from gymnasium.spaces import Box, Discrete\n",
    "import torch.nn as nn\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib\n",
    "import torch\n",
    "import itertools\n",
    "import torch.nn.functional as F"
   ],
   "outputs": [],
   "execution_count": 29
  },
  {
   "cell_type": "markdown",
   "id": "13ce1be1",
   "metadata": {},
   "source": [
    "# custom game environment"
   ]
  },
  {
   "cell_type": "code",
   "id": "baf7c84d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T15:46:20.982238Z",
     "start_time": "2025-07-23T15:46:20.937266Z"
    }
   },
   "source": [
    "class MrDino(Env):\n",
    "    def __init__(self, render_mode):\n",
    "        super().__init__()\n",
    "        # setup spaces\n",
    "        self.render_mode = render_mode\n",
    "        self.observation_space=Box(low=0,high=255, shape=(1,83,100), dtype=np.uint8)\n",
    "        self.action_space = Discrete(3) #number of action\n",
    "\n",
    "        self.cap =mss()\n",
    "\n",
    "        self.obstacle_passed_region = {'top': 245, 'left': 570, 'width': 38, 'height': 135}\n",
    "        self.obstacle_coming = {'top':205, 'left':647, 'width':273, 'height':175} #(30,270) (h,w)\n",
    "        self.game_location = {'top':205, 'left':580, 'width':330, 'height':175} #(30,270) (h,w)\n",
    "        self.done_location = {'top':240, 'left':820, 'width':280, 'height':50}\n",
    "\n",
    "    def step(self, action):\n",
    "        # action key: 0= space, 1=down, 2=no action\n",
    "        action_map ={\n",
    "            0: 'space',\n",
    "            1: 'down',\n",
    "            2: 'no_op'\n",
    "        }\n",
    "\n",
    "        if action !=2:\n",
    "            pydirectinput.press(action_map[action])\n",
    "\n",
    "        res,  done, done_cap=self.get_done()\n",
    "        new_observation =self.get_observation()\n",
    "        \n",
    "        \n",
    "        if done:\n",
    "            reward = -10  # penalty if game over\n",
    "        else:\n",
    "            reward= +1\n",
    "        if self.obstacle_passed():\n",
    "            reward += 0.1   # reward for surviving a frame\n",
    "\n",
    "        # info dictionary\n",
    "        info = {}\n",
    "        truncated= False\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "           self.render()\n",
    "           \n",
    "        return new_observation, reward, done, truncated, info\n",
    "\n",
    "    def render(self): #visualize the game\n",
    "        # cv2.imshow('Game', np.array(self.cap.grab(self.game_location))[:,:,:3])\n",
    "        region_img = np.array(self.cap.grab(self.game_location))[:, :, :3]\n",
    "    \n",
    "        gray = cv2.cvtColor(region_img, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        threshold_value = 170  # ← tweak this\n",
    "        _, thresh = cv2.threshold(gray, threshold_value, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "        cv2.imshow('Thresholded', thresh) \n",
    "        if cv2.waitKey(0) & 0xFF == ord('q'):\n",
    "            self.close()\n",
    "\n",
    "    def reset(self, *, seed=None, options=None): \n",
    "       super().reset(seed=seed)\n",
    "       time.sleep(0.1 )\n",
    "       pydirectinput.click(x=150,y=300)\n",
    "       pydirectinput.press('space')\n",
    "       obs = self.get_observation()\n",
    "       info = {}\n",
    "       if self.render_mode == \"human\":\n",
    "           self.render()\n",
    "       return obs, info\n",
    "\n",
    "    def close(self):\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "    def get_observation(self):\n",
    "        raw = np.array(self.cap.grab(self.game_location))[:, :, :3].astype(np.uint8)\n",
    "        gray = cv2.cvtColor(raw, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        _, thresh = cv2.threshold(gray, 170, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "        resized = cv2.resize(thresh, (100, 83))  # (width, height)\n",
    "        # add channel dimension\n",
    "        channel = np.reshape(resized, (1, 83, 100))\n",
    "        obstacle_ahead= self.obstacle_ahead()\n",
    "        return channel, obstacle_ahead\n",
    "\n",
    "    def obstacle_ahead(self):\n",
    "        region_img = np.array(self.cap.grab(self.obstacle_coming))[:, :, :3]\n",
    "        gray = cv2.cvtColor(region_img, cv2.COLOR_BGR2GRAY)\n",
    "        _, thresh = cv2.threshold(gray, 170, 255, cv2.THRESH_BINARY)\n",
    "        resized = cv2.resize(thresh, (100, 83))\n",
    "        obstacle_pixels = np.sum(resized == 255)\n",
    "\n",
    "        return (obstacle_pixels > 25).astype(np.int8)\n",
    "\n",
    "    \n",
    "    def get_done(self):\n",
    "        pytesseract.pytesseract.tesseract_cmd = r\"C:\\Program Files\\Tesseract-OCR\\tesseract.exe\"\n",
    "        # game over text extraction\n",
    "        done_cap= np.array(self.cap.grab(self.done_location))[:,:,:3]\n",
    "        done_strings= ['GAME', 'GAHE']\n",
    "        \n",
    "        done =False\n",
    "        res= pytesseract.image_to_string(done_cap)[:4]\n",
    "        if res in done_strings:\n",
    "            done= True\n",
    "\n",
    "        return res, done, done_cap\n",
    "    \n",
    "    def obstacle_passed(self):\n",
    "        region_img = np.array(self.cap.grab(self.obstacle_passed_region))[:, :, :3]\n",
    "        gray = cv2.cvtColor(region_img, cv2.COLOR_BGR2GRAY)\n",
    "        _, thresh = cv2.threshold(gray, 170, 255, cv2.THRESH_BINARY)\n",
    "        obstacle_pixels = np.sum(thresh == 255)\n",
    "\n",
    "     \n",
    "        return obstacle_pixels > 350  # adjust based on what you observe"
   ],
   "outputs": [],
   "execution_count": 30
  },
  {
   "cell_type": "code",
   "id": "21c21286",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T15:46:21.002559Z",
     "start_time": "2025-07-23T15:46:20.996013Z"
    }
   },
   "source": [
    "env= MrDino(render_mode=None)"
   ],
   "outputs": [],
   "execution_count": 31
  },
  {
   "cell_type": "code",
   "id": "824d9f92fe9e9e17",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T15:46:21.241028Z",
     "start_time": "2025-07-23T15:46:21.048006Z"
    }
   },
   "source": [
    "plt.imshow(cv2.cvtColor(env.get_observation()[0][0], cv2.COLOR_BGR2RGB))"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x236eb8060d0>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 32
  },
  {
   "cell_type": "code",
   "id": "8063c963a67ac92b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T15:46:21.327624Z",
     "start_time": "2025-07-23T15:46:21.256155Z"
    }
   },
   "source": [
    "env.get_observation()[0].shape"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 83, 100)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 33
  },
  {
   "cell_type": "markdown",
   "id": "99d0b98ef7646846",
   "metadata": {},
   "source": [
    "# replay memory"
   ]
  },
  {
   "cell_type": "code",
   "id": "f237cae0f365f79a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T15:46:21.454944Z",
     "start_time": "2025-07-23T15:46:21.429149Z"
    }
   },
   "source": [
    "from collections import deque\n",
    "import random\n",
    "class ReplayMemory():\n",
    "    def __init__(self, maxlen, seed=None):\n",
    "        self.memory = deque([], maxlen=maxlen)\n",
    "\n",
    "        # Optional seed for reproducibility\n",
    "        if seed is not None:\n",
    "            random.seed(seed)\n",
    "\n",
    "    def append(self, transition):\n",
    "        self.memory.append(transition)\n",
    "\n",
    "    def sample(self, sample_size):\n",
    "        return random.sample(self.memory, sample_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ],
   "outputs": [],
   "execution_count": 34
  },
  {
   "cell_type": "markdown",
   "id": "6ec4cd37beead13c",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "id": "c1b93eecdf28b7c2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T15:46:21.691811Z",
     "start_time": "2025-07-23T15:46:21.611380Z"
    }
   },
   "source": [
    "\n",
    "class CNN_DQN(nn.Module):\n",
    "    def __init__(self, image_height, image_width, action_dim=3,  # 3 actions for dino\n",
    "                 hidden_dim=128, enable_dueling_dqn=True):  # Reduced from 256\n",
    "        super(CNN_DQN, self).__init__()\n",
    "\n",
    "        self.enable_dueling_dqn = enable_dueling_dqn\n",
    "\n",
    "        # Lighter CNN for simple game graphics\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=8, stride=4, padding=2)  # Reduced filters\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=4, stride=2, padding=1)  # Reduced filters\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)  # Reduced filters\n",
    "\n",
    "        # Calculate the size of flattened CNN output\n",
    "        self.cnn_output_size = self._get_cnn_output_size(1, image_height, image_width)\n",
    "\n",
    "        # Simpler FC layers - sufficient for dino game\n",
    "        self.fc1 = nn.Linear(self.cnn_output_size + 1, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim // 2)  # Only 2 shared layers\n",
    "\n",
    "        # Lighter dropout\n",
    "        self.dropout = nn.Dropout(0.2)  # Reduced from 0.3\n",
    "\n",
    "        if self.enable_dueling_dqn:\n",
    "            # Simpler dueling streams\n",
    "            self.fc_value = nn.Linear(hidden_dim // 2, 64)\n",
    "            self.value = nn.Linear(64, 1)\n",
    "\n",
    "            self.fc_advantages = nn.Linear(hidden_dim // 2, 64)\n",
    "            self.advantages = nn.Linear(64, action_dim)\n",
    "        else:\n",
    "            # Standard DQN\n",
    "            self.fc3 = nn.Linear(hidden_dim // 2, 64)\n",
    "            self.output = nn.Linear(64, action_dim)\n",
    "\n",
    "    def _get_cnn_output_size(self, channels, height, width):\n",
    "        \"\"\"Calculate the output size of CNN layers\"\"\"\n",
    "        with torch.no_grad():\n",
    "            dummy_input = torch.zeros(1, channels, height, width)\n",
    "            x = F.relu(self.conv1(dummy_input))\n",
    "            x = F.relu(self.conv2(x))\n",
    "            x = F.relu(self.conv3(x))\n",
    "            # print(f\"cnn output size{x.flatten(1).size(1)}\")\n",
    "            return x.flatten(1).size(1)\n",
    "\n",
    "    def forward(self, image_input, integer_input):\n",
    "        # Process image through CNN\n",
    "        x = F.relu(self.conv1(image_input))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "\n",
    "        # Flatten CNN output\n",
    "        x = x.view(x.size(0), -1)\n",
    "        # print(x.shape)\n",
    "\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        # Convert to tensor if it's not already\n",
    "        if not isinstance(integer_input, torch.Tensor):\n",
    "            integer_input = torch.tensor(integer_input, dtype=torch.float32)\n",
    "\n",
    "        # print(f\"Integer input before processing: {integer_input.shape if hasattr(integer_input, 'shape') else integer_input}\")\n",
    "\n",
    "        if integer_input.dim() == 0:  # scalar tensor\n",
    "            # Expand to match batch size\n",
    "            integer_input = integer_input.unsqueeze(0).expand(batch_size, 1)\n",
    "        elif integer_input.dim() == 1:  # 1D tensor\n",
    "            if integer_input.size(0) == 1 and batch_size > 1:\n",
    "                # Single value, expand to match batch size\n",
    "                integer_input = integer_input.expand(batch_size).unsqueeze(1)\n",
    "            elif integer_input.size(0) == batch_size:\n",
    "                # Already correct batch size, just add feature dimension\n",
    "                integer_input = integer_input.unsqueeze(1)\n",
    "            else:\n",
    "                # Mismatch - take first element and expand\n",
    "                integer_input = integer_input[0].unsqueeze(0).expand(batch_size, 1)\n",
    "        elif integer_input.dim() == 2:\n",
    "            # Already has correct shape, just verify batch size\n",
    "            if integer_input.size(0) != batch_size:\n",
    "                # Take first row and expand\n",
    "                integer_input = integer_input[0:1].expand(batch_size, -1)\n",
    "\n",
    "        # print(f\"Integer input after processing: {integer_input.shape}\")\n",
    "        # print(f\"CNN features shape: {x.shape}\")\n",
    "\n",
    "        # Concatenate CNN features with integer input\n",
    "        combined_input = torch.cat([x, integer_input.float()], dim=1)\n",
    "\n",
    "        # Simplified FC processing\n",
    "        x = F.relu(self.fc1(combined_input))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "\n",
    "        if self.enable_dueling_dqn:\n",
    "            # Value stream\n",
    "            v = F.relu(self.fc_value(x))\n",
    "            V = self.value(v)\n",
    "\n",
    "            # Advantage stream\n",
    "            a = F.relu(self.fc_advantages(x))\n",
    "            A = self.advantages(a)\n",
    "\n",
    "            # Calculate Q-values\n",
    "            Q = V + A - torch.mean(A, dim=1, keepdim=True)\n",
    "        else:\n",
    "            # Standard DQN\n",
    "            x = F.relu(self.fc3(x))\n",
    "            Q = self.output(x)\n",
    "\n",
    "        return Q\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 35
  },
  {
   "cell_type": "code",
   "id": "6707b82987dba53a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T15:46:21.829687Z",
     "start_time": "2025-07-23T15:46:21.761396Z"
    }
   },
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "DATE_FORMAT = \"%m-%d %H:%M:%S\"\n",
    "\n",
    "# Directory for saving run info\n",
    "RUNS_DIR = \"runs\"\n",
    "os.makedirs(RUNS_DIR, exist_ok=True)\n",
    "\n",
    "# 'Agg': used to generate plots as images and save them to a file instead of rendering to screen\n",
    "matplotlib.use('Agg')\n",
    "\n",
    "class Agent():\n",
    "    def __init__(self):\n",
    "        self.learning_rate_a    = 0.001        # learning rate (alpha)\n",
    "        self.discount_factor_g  = 0.99     # discount rate (gamma)\n",
    "        self.network_sync_rate  = 100      # number of steps the agent takes before syncing the policy and target network\n",
    "        self.replay_memory_size = 100000     # size of replay memory\n",
    "        self.mini_batch_size    = 64        # size of the training data set sampled from the replay memory\n",
    "        self.epsilon_init       = 1           # 1 = 100% random actions\n",
    "        self.epsilon_decay      = 0.9995          # epsilon decay rate\n",
    "        self.epsilon_min        = 0.01\n",
    "        self.enable_double_dqn  = True\n",
    "        self.enable_dueling_dqn = True\n",
    "        self.stop_on_reward = 10000\n",
    "\n",
    "        self.loss_fn = nn.MSELoss()          # NN Loss function. MSE=Mean Squared Error can be swapped to something else.\n",
    "        self.optimizer = None\n",
    "\n",
    "        self.LOG_FILE   = os.path.join(RUNS_DIR, f'{\"MrDino\"}.log')\n",
    "        self.MODEL_FILE = os.path.join(RUNS_DIR, f'{\"MrDino\"}.pt')\n",
    "        self.GRAPH_FILE = os.path.join(RUNS_DIR, f'{\"MrDino\"}.png')\n",
    "\n",
    "    def run(self, is_training=True, render=False):\n",
    "        if is_training:\n",
    "            start_time = datetime.now()\n",
    "            last_graph_update_time = start_time\n",
    "\n",
    "            log_message = f\"{start_time.strftime(DATE_FORMAT)}: Training starting...\"\n",
    "            print(log_message)\n",
    "            with open(self.LOG_FILE, 'w') as file:\n",
    "                file.write(log_message + '\\n')\n",
    "        env =MrDino(render_mode= None)\n",
    "        num_actions = env.action_space.n\n",
    "        rewards_per_episode = []\n",
    "\n",
    "        # Create policy and target network. Number of nodes in the hidden layer can be adjusted.\n",
    "        policy_dqn = CNN_DQN(83,100,num_actions, 256,self.enable_dueling_dqn).to(device)\n",
    "\n",
    "        if is_training:\n",
    "            epsilon = self.epsilon_init\n",
    "\n",
    "            memory = ReplayMemory(self.replay_memory_size)\n",
    "\n",
    "            target_dqn = CNN_DQN(83,100,num_actions ,256,self.enable_dueling_dqn).to(device)\n",
    "            target_dqn.load_state_dict(policy_dqn.state_dict())\n",
    "\n",
    "            self.optimizer = torch.optim.Adam(policy_dqn.parameters(), lr=self.learning_rate_a)\n",
    "\n",
    "            epsilon_history = []\n",
    "\n",
    "            step_count=0\n",
    "\n",
    "            best_reward = -9999999\n",
    "        else:\n",
    "            policy_dqn.load_state_dict(torch.load(self.MODEL_FILE))\n",
    "\n",
    "            policy_dqn.eval()\n",
    "\n",
    "        for episode in itertools.count():\n",
    "\n",
    "            state, _ = env.reset()  # Initialize environment. Reset returns (state,info).\n",
    "            state = (\n",
    "                torch.tensor(state[0], dtype=torch.float, device=device),   # binary image\n",
    "                torch.tensor(state[1], dtype=torch.float, device=device)    # 0 or 1 flag\n",
    "            )\n",
    "\n",
    "\n",
    "            terminated = False     \n",
    "            episode_reward = 0.0    \n",
    "\n",
    "           \n",
    "            while(not terminated and episode_reward < self.stop_on_reward):\n",
    "\n",
    "                # Select action based on epsilon-greedy\n",
    "                if is_training and random.random() < epsilon:\n",
    "                    action = env.action_space.sample()\n",
    "                    action = torch.tensor(action, dtype=torch.int64, device=device)\n",
    "                else:\n",
    "                    with torch.no_grad():\n",
    "                        action = policy_dqn(state[0].unsqueeze(0) if state[0].dim() == 3 else state[0],state[1].unsqueeze(0) if state[1].dim() == 1 else state[1]).argmax()\n",
    "\n",
    "                # Execute action. Truncated and info is not used.\n",
    "                new_state,reward,terminated,truncated,info = env.step(action.item())\n",
    "\n",
    "                # Accumulate rewards\n",
    "                episode_reward += reward\n",
    "\n",
    "                # Convert new state and reward to tensors on device\n",
    "                new_state = (torch.tensor(new_state[0], dtype=torch.float, device=device),   # binary image\n",
    "                             torch.tensor(new_state[1], dtype=torch.float, device=device) )\n",
    "                reward = torch.tensor(reward, dtype=torch.float, device=device)\n",
    "\n",
    "                if is_training:\n",
    "                    # Save experience into memory\n",
    "                    memory.append((state, action, new_state, reward, terminated))\n",
    "\n",
    "                    # Increment step counter\n",
    "                    step_count+=1\n",
    "\n",
    "                # Move to the next state\n",
    "                state = new_state\n",
    "\n",
    "            # Keep track of the rewards collected per episode.\n",
    "            rewards_per_episode.append(episode_reward)\n",
    "\n",
    "            # Save model when new best reward is obtained.\n",
    "            if is_training:\n",
    "                if episode_reward > best_reward:\n",
    "                    log_message = f\"{datetime.now().strftime(DATE_FORMAT)}: New best reward {episode_reward:0.1f} ({(episode_reward-best_reward)/best_reward*100:+.1f}%) at episode {episode}, saving model...\" if best_reward !=0 else f\"{datetime.now().strftime(DATE_FORMAT)}: New best reward {episode_reward:0.1f} (0%) at episode {episode}, saving model...\"\n",
    "                    print(log_message)\n",
    "                    with open(self.LOG_FILE, 'a') as file:\n",
    "                        file.write(log_message + '\\n')\n",
    "\n",
    "                    torch.save(policy_dqn.state_dict(), self.MODEL_FILE)\n",
    "                    best_reward = episode_reward\n",
    "\n",
    "\n",
    "                # Update graph every x seconds\n",
    "                current_time = datetime.now()\n",
    "                if current_time - last_graph_update_time > timedelta(seconds=10):\n",
    "                    self.save_graph(rewards_per_episode, epsilon_history)\n",
    "                    last_graph_update_time = current_time\n",
    "\n",
    "                # If enough experience has been collected\n",
    "                if len(memory)>self.mini_batch_size:\n",
    "                    mini_batch = memory.sample(self.mini_batch_size)\n",
    "                    self.optimize(mini_batch, policy_dqn, target_dqn)\n",
    "\n",
    "                    # Decay epsilon\n",
    "                    epsilon = max(epsilon * self.epsilon_decay, self.epsilon_min)\n",
    "                    epsilon_history.append(epsilon)\n",
    "\n",
    "                    # Copy policy network to target network after a certain number of steps\n",
    "                    if step_count > self.network_sync_rate:\n",
    "                        target_dqn.load_state_dict(policy_dqn.state_dict())\n",
    "                        step_count=0\n",
    "\n",
    "    def save_graph(self, rewards_per_episode, epsilon_history):\n",
    "        # Save plots\n",
    "        fig = plt.figure(1)\n",
    "\n",
    "        # Plot average rewards (Y-axis) vs episodes (X-axis)\n",
    "        mean_rewards = np.zeros(len(rewards_per_episode))\n",
    "        for x in range(len(mean_rewards)):\n",
    "            mean_rewards[x] = np.mean(rewards_per_episode[max(0, x-99):(x+1)])\n",
    "        plt.subplot(121) # plot on a 1 row x 2 col grid, at cell 1\n",
    "        # plt.xlabel('Episodes')\n",
    "        plt.ylabel('Mean Rewards')\n",
    "        plt.plot(mean_rewards)\n",
    "\n",
    "        # Plot epsilon decay (Y-axis) vs episodes (X-axis)\n",
    "        plt.subplot(122) # plot on a 1 row x 2 col grid, at cell 2\n",
    "        # plt.xlabel('Time Steps')\n",
    "        plt.ylabel('Epsilon Decay')\n",
    "        plt.plot(epsilon_history)\n",
    "\n",
    "        plt.subplots_adjust(wspace=1.0, hspace=1.0)\n",
    "\n",
    "        # Save plots\n",
    "        fig.savefig(self.GRAPH_FILE)\n",
    "        plt.close(fig)\n",
    "\n",
    "    def optimize(self, mini_batch, policy_dqn, target_dqn):\n",
    "\n",
    "        states, actions, new_states, rewards, terminations = zip(*mini_batch)\n",
    "\n",
    "        # Unzip image and flag parts from state and new_state\n",
    "        state_imgs, state_flags = zip(*states)\n",
    "        new_state_imgs, new_state_flags = zip(*new_states)\n",
    "\n",
    "        # Stack each part separately\n",
    "        state_imgs = torch.stack(state_imgs)         # shape [B, C, H, W] or [B, H, W]\n",
    "        state_flags = torch.stack(state_flags)       # shape [B] or [B, 1]\n",
    "\n",
    "        new_state_imgs = torch.stack(new_state_imgs)\n",
    "        new_state_flags = torch.stack(new_state_flags)\n",
    "\n",
    "        actions = torch.stack(actions)\n",
    "\n",
    "        rewards = torch.stack(rewards)\n",
    "        terminations = torch.tensor(terminations).float().to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            if self.enable_double_dqn:\n",
    "                best_actions_from_policy = policy_dqn(new_state_imgs,new_state_flags).argmax(dim=1)\n",
    "\n",
    "                target_q = rewards + (1-terminations) * self.discount_factor_g * \\\n",
    "                                target_dqn(new_state_imgs,new_state_flags).gather(dim=1, index=best_actions_from_policy.unsqueeze(dim=1)).squeeze()\n",
    "            else:\n",
    "                # Calculate target Q values (expected returns)\n",
    "                target_q = rewards + (1-terminations) * self.discount_factor_g * target_dqn(new_states).max(dim=1)[0]\n",
    "            #    (1-terminal) is done becuase if the event is terminated future Q-value is ignored\n",
    "\n",
    "        # Calcuate Q values from current policy\n",
    "        current_q = policy_dqn(state_imgs, state_flags).gather(dim=1, index=actions.unsqueeze(dim=1)).squeeze()\n",
    "\n",
    "\n",
    "        # Compute loss\n",
    "        loss = self.loss_fn(current_q, target_q)\n",
    "\n",
    "        # Optimize the model (backpropagation)\n",
    "        self.optimizer.zero_grad()  # Clear gradients\n",
    "        loss.backward()             # Compute gradients\n",
    "        self.optimizer.step()\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 36
  },
  {
   "cell_type": "code",
   "id": "e7ff9e505e40d7f9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T15:52:26.872910Z",
     "start_time": "2025-07-23T15:46:21.989679Z"
    }
   },
   "source": [
    "dql = Agent()\n",
    "\n",
    "dql.run(is_training=False)\n"
   ],
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[37]\u001B[39m\u001B[32m, line 3\u001B[39m\n\u001B[32m      1\u001B[39m dql = Agent()\n\u001B[32m----> \u001B[39m\u001B[32m3\u001B[39m \u001B[43mdql\u001B[49m\u001B[43m.\u001B[49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[43mis_training\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[36]\u001B[39m\u001B[32m, line 93\u001B[39m, in \u001B[36mAgent.run\u001B[39m\u001B[34m(self, is_training, render)\u001B[39m\n\u001B[32m     90\u001B[39m         action = policy_dqn(state[\u001B[32m0\u001B[39m].unsqueeze(\u001B[32m0\u001B[39m) \u001B[38;5;28;01mif\u001B[39;00m state[\u001B[32m0\u001B[39m].dim() == \u001B[32m3\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m state[\u001B[32m0\u001B[39m],state[\u001B[32m1\u001B[39m].unsqueeze(\u001B[32m0\u001B[39m) \u001B[38;5;28;01mif\u001B[39;00m state[\u001B[32m1\u001B[39m].dim() == \u001B[32m1\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m state[\u001B[32m1\u001B[39m]).argmax()\n\u001B[32m     92\u001B[39m \u001B[38;5;66;03m# Execute action. Truncated and info is not used.\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m93\u001B[39m new_state,reward,terminated,truncated,info = \u001B[43menv\u001B[49m\u001B[43m.\u001B[49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction\u001B[49m\u001B[43m.\u001B[49m\u001B[43mitem\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     95\u001B[39m \u001B[38;5;66;03m# Accumulate rewards\u001B[39;00m\n\u001B[32m     96\u001B[39m episode_reward += reward\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[30]\u001B[39m\u001B[32m, line 27\u001B[39m, in \u001B[36mMrDino.step\u001B[39m\u001B[34m(self, action)\u001B[39m\n\u001B[32m     24\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m action !=\u001B[32m2\u001B[39m:\n\u001B[32m     25\u001B[39m     pydirectinput.press(action_map[action])\n\u001B[32m---> \u001B[39m\u001B[32m27\u001B[39m res,  done, done_cap=\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mget_done\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     28\u001B[39m new_observation =\u001B[38;5;28mself\u001B[39m.get_observation()\n\u001B[32m     31\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m done:\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[30]\u001B[39m\u001B[32m, line 103\u001B[39m, in \u001B[36mMrDino.get_done\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    100\u001B[39m done_strings= [\u001B[33m'\u001B[39m\u001B[33mGAME\u001B[39m\u001B[33m'\u001B[39m, \u001B[33m'\u001B[39m\u001B[33mGAHE\u001B[39m\u001B[33m'\u001B[39m]\n\u001B[32m    102\u001B[39m done =\u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m103\u001B[39m res= \u001B[43mpytesseract\u001B[49m\u001B[43m.\u001B[49m\u001B[43mimage_to_string\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdone_cap\u001B[49m\u001B[43m)\u001B[49m[:\u001B[32m4\u001B[39m]\n\u001B[32m    104\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m res \u001B[38;5;129;01min\u001B[39;00m done_strings:\n\u001B[32m    105\u001B[39m     done= \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mc:\\Users\\indra\\AppData\\Local\\anaconda3\\envs\\dqnenv\\Lib\\site-packages\\pytesseract\\pytesseract.py:486\u001B[39m, in \u001B[36mimage_to_string\u001B[39m\u001B[34m(image, lang, config, nice, output_type, timeout)\u001B[39m\n\u001B[32m    481\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    482\u001B[39m \u001B[33;03mReturns the result of a Tesseract OCR run on the provided image to string\u001B[39;00m\n\u001B[32m    483\u001B[39m \u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    484\u001B[39m args = [image, \u001B[33m'\u001B[39m\u001B[33mtxt\u001B[39m\u001B[33m'\u001B[39m, lang, config, nice, timeout]\n\u001B[32m--> \u001B[39m\u001B[32m486\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m{\u001B[49m\n\u001B[32m    487\u001B[39m \u001B[43m    \u001B[49m\u001B[43mOutput\u001B[49m\u001B[43m.\u001B[49m\u001B[43mBYTES\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mlambda\u001B[39;49;00m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mrun_and_get_output\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43m(\u001B[49m\u001B[43margs\u001B[49m\u001B[43m \u001B[49m\u001B[43m+\u001B[49m\u001B[43m \u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    488\u001B[39m \u001B[43m    \u001B[49m\u001B[43mOutput\u001B[49m\u001B[43m.\u001B[49m\u001B[43mDICT\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mlambda\u001B[39;49;00m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43m{\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mtext\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mrun_and_get_output\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    489\u001B[39m \u001B[43m    \u001B[49m\u001B[43mOutput\u001B[49m\u001B[43m.\u001B[49m\u001B[43mSTRING\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mlambda\u001B[39;49;00m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mrun_and_get_output\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    490\u001B[39m \u001B[43m\u001B[49m\u001B[43m}\u001B[49m\u001B[43m[\u001B[49m\u001B[43moutput_type\u001B[49m\u001B[43m]\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mc:\\Users\\indra\\AppData\\Local\\anaconda3\\envs\\dqnenv\\Lib\\site-packages\\pytesseract\\pytesseract.py:489\u001B[39m, in \u001B[36mimage_to_string.<locals>.<lambda>\u001B[39m\u001B[34m()\u001B[39m\n\u001B[32m    481\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    482\u001B[39m \u001B[33;03mReturns the result of a Tesseract OCR run on the provided image to string\u001B[39;00m\n\u001B[32m    483\u001B[39m \u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    484\u001B[39m args = [image, \u001B[33m'\u001B[39m\u001B[33mtxt\u001B[39m\u001B[33m'\u001B[39m, lang, config, nice, timeout]\n\u001B[32m    486\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m {\n\u001B[32m    487\u001B[39m     Output.BYTES: \u001B[38;5;28;01mlambda\u001B[39;00m: run_and_get_output(*(args + [\u001B[38;5;28;01mTrue\u001B[39;00m])),\n\u001B[32m    488\u001B[39m     Output.DICT: \u001B[38;5;28;01mlambda\u001B[39;00m: {\u001B[33m'\u001B[39m\u001B[33mtext\u001B[39m\u001B[33m'\u001B[39m: run_and_get_output(*args)},\n\u001B[32m--> \u001B[39m\u001B[32m489\u001B[39m     Output.STRING: \u001B[38;5;28;01mlambda\u001B[39;00m: \u001B[43mrun_and_get_output\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m,\n\u001B[32m    490\u001B[39m }[output_type]()\n",
      "\u001B[36mFile \u001B[39m\u001B[32mc:\\Users\\indra\\AppData\\Local\\anaconda3\\envs\\dqnenv\\Lib\\site-packages\\pytesseract\\pytesseract.py:352\u001B[39m, in \u001B[36mrun_and_get_output\u001B[39m\u001B[34m(image, extension, lang, config, nice, timeout, return_bytes)\u001B[39m\n\u001B[32m    341\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m save(image) \u001B[38;5;28;01mas\u001B[39;00m (temp_name, input_filename):\n\u001B[32m    342\u001B[39m     kwargs = {\n\u001B[32m    343\u001B[39m         \u001B[33m'\u001B[39m\u001B[33minput_filename\u001B[39m\u001B[33m'\u001B[39m: input_filename,\n\u001B[32m    344\u001B[39m         \u001B[33m'\u001B[39m\u001B[33moutput_filename_base\u001B[39m\u001B[33m'\u001B[39m: temp_name,\n\u001B[32m   (...)\u001B[39m\u001B[32m    349\u001B[39m         \u001B[33m'\u001B[39m\u001B[33mtimeout\u001B[39m\u001B[33m'\u001B[39m: timeout,\n\u001B[32m    350\u001B[39m     }\n\u001B[32m--> \u001B[39m\u001B[32m352\u001B[39m     \u001B[43mrun_tesseract\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    353\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m _read_output(\n\u001B[32m    354\u001B[39m         \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mkwargs[\u001B[33m'\u001B[39m\u001B[33moutput_filename_base\u001B[39m\u001B[33m'\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00mextsep\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00mextension\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    355\u001B[39m         return_bytes,\n\u001B[32m    356\u001B[39m     )\n",
      "\u001B[36mFile \u001B[39m\u001B[32mc:\\Users\\indra\\AppData\\Local\\anaconda3\\envs\\dqnenv\\Lib\\site-packages\\pytesseract\\pytesseract.py:282\u001B[39m, in \u001B[36mrun_tesseract\u001B[39m\u001B[34m(input_filename, output_filename_base, extension, lang, config, nice, timeout)\u001B[39m\n\u001B[32m    279\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    280\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m TesseractNotFoundError()\n\u001B[32m--> \u001B[39m\u001B[32m282\u001B[39m \u001B[43m\u001B[49m\u001B[38;5;28;43;01mwith\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mtimeout_manager\u001B[49m\u001B[43m(\u001B[49m\u001B[43mproc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mas\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43merror_string\u001B[49m\u001B[43m:\u001B[49m\n\u001B[32m    283\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mproc\u001B[49m\u001B[43m.\u001B[49m\u001B[43mreturncode\u001B[49m\u001B[43m:\u001B[49m\n\u001B[32m    284\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43;01mraise\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mTesseractError\u001B[49m\u001B[43m(\u001B[49m\u001B[43mproc\u001B[49m\u001B[43m.\u001B[49m\u001B[43mreturncode\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mget_errors\u001B[49m\u001B[43m(\u001B[49m\u001B[43merror_string\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mc:\\Users\\indra\\AppData\\Local\\anaconda3\\envs\\dqnenv\\Lib\\contextlib.py:137\u001B[39m, in \u001B[36m_GeneratorContextManager.__enter__\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    135\u001B[39m \u001B[38;5;28;01mdel\u001B[39;00m \u001B[38;5;28mself\u001B[39m.args, \u001B[38;5;28mself\u001B[39m.kwds, \u001B[38;5;28mself\u001B[39m.func\n\u001B[32m    136\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m137\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mnext\u001B[39m(\u001B[38;5;28mself\u001B[39m.gen)\n\u001B[32m    138\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mStopIteration\u001B[39;00m:\n\u001B[32m    139\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[33m\"\u001B[39m\u001B[33mgenerator didn\u001B[39m\u001B[33m'\u001B[39m\u001B[33mt yield\u001B[39m\u001B[33m\"\u001B[39m) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mc:\\Users\\indra\\AppData\\Local\\anaconda3\\envs\\dqnenv\\Lib\\site-packages\\pytesseract\\pytesseract.py:144\u001B[39m, in \u001B[36mtimeout_manager\u001B[39m\u001B[34m(proc, seconds)\u001B[39m\n\u001B[32m    142\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m    143\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m seconds:\n\u001B[32m--> \u001B[39m\u001B[32m144\u001B[39m         \u001B[38;5;28;01myield\u001B[39;00m \u001B[43mproc\u001B[49m\u001B[43m.\u001B[49m\u001B[43mcommunicate\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m[\u001B[32m1\u001B[39m]\n\u001B[32m    145\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m\n\u001B[32m    147\u001B[39m     \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32mc:\\Users\\indra\\AppData\\Local\\anaconda3\\envs\\dqnenv\\Lib\\subprocess.py:1209\u001B[39m, in \u001B[36mPopen.communicate\u001B[39m\u001B[34m(self, input, timeout)\u001B[39m\n\u001B[32m   1206\u001B[39m     endtime = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1208\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1209\u001B[39m     stdout, stderr = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_communicate\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mendtime\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1210\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyboardInterrupt\u001B[39;00m:\n\u001B[32m   1211\u001B[39m     \u001B[38;5;66;03m# https://bugs.python.org/issue25942\u001B[39;00m\n\u001B[32m   1212\u001B[39m     \u001B[38;5;66;03m# See the detailed comment in .wait().\u001B[39;00m\n\u001B[32m   1213\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m timeout \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32mc:\\Users\\indra\\AppData\\Local\\anaconda3\\envs\\dqnenv\\Lib\\subprocess.py:1628\u001B[39m, in \u001B[36mPopen._communicate\u001B[39m\u001B[34m(self, input, endtime, orig_timeout)\u001B[39m\n\u001B[32m   1624\u001B[39m \u001B[38;5;66;03m# Wait for the reader threads, or time out.  If we time out, the\u001B[39;00m\n\u001B[32m   1625\u001B[39m \u001B[38;5;66;03m# threads remain reading and the fds left open in case the user\u001B[39;00m\n\u001B[32m   1626\u001B[39m \u001B[38;5;66;03m# calls communicate again.\u001B[39;00m\n\u001B[32m   1627\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.stdout \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1628\u001B[39m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mstdout_thread\u001B[49m\u001B[43m.\u001B[49m\u001B[43mjoin\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_remaining_time\u001B[49m\u001B[43m(\u001B[49m\u001B[43mendtime\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1629\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.stdout_thread.is_alive():\n\u001B[32m   1630\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m TimeoutExpired(\u001B[38;5;28mself\u001B[39m.args, orig_timeout)\n",
      "\u001B[36mFile \u001B[39m\u001B[32mc:\\Users\\indra\\AppData\\Local\\anaconda3\\envs\\dqnenv\\Lib\\threading.py:1119\u001B[39m, in \u001B[36mThread.join\u001B[39m\u001B[34m(self, timeout)\u001B[39m\n\u001B[32m   1116\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[33m\"\u001B[39m\u001B[33mcannot join current thread\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m   1118\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m timeout \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1119\u001B[39m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_wait_for_tstate_lock\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1120\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   1121\u001B[39m     \u001B[38;5;66;03m# the behavior of a negative timeout isn't documented, but\u001B[39;00m\n\u001B[32m   1122\u001B[39m     \u001B[38;5;66;03m# historically .join(timeout=x) for x<0 has acted as if timeout=0\u001B[39;00m\n\u001B[32m   1123\u001B[39m     \u001B[38;5;28mself\u001B[39m._wait_for_tstate_lock(timeout=\u001B[38;5;28mmax\u001B[39m(timeout, \u001B[32m0\u001B[39m))\n",
      "\u001B[36mFile \u001B[39m\u001B[32mc:\\Users\\indra\\AppData\\Local\\anaconda3\\envs\\dqnenv\\Lib\\threading.py:1139\u001B[39m, in \u001B[36mThread._wait_for_tstate_lock\u001B[39m\u001B[34m(self, block, timeout)\u001B[39m\n\u001B[32m   1136\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m\n\u001B[32m   1138\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1139\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[43mlock\u001B[49m\u001B[43m.\u001B[49m\u001B[43macquire\u001B[49m\u001B[43m(\u001B[49m\u001B[43mblock\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m:\n\u001B[32m   1140\u001B[39m         lock.release()\n\u001B[32m   1141\u001B[39m         \u001B[38;5;28mself\u001B[39m._stop()\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "cell_type": "code",
   "id": "1b3e0bae7a8acc3b",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dqnenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
